user_input,reference_contexts,reference,synthesizer_name
What is SPO in the context of MetaGPT and what does it involve?,"['# MetaGPT: The Multi-Agent Framework\n\n<p align=""center"">\n<a href=""""><img src=""docs/resources/MetaGPT-new-log.png"" alt=""MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks."" width=""150px""></a>\n</p>\n\n<p align=""center"">\n[ <b>En</b> |\n<a href=""docs/README_CN.md"">‰∏≠</a> |\n<a href=""docs/README_FR.md"">Fr</a> |\n<a href=""docs/README_JA.md"">Êó•</a> ]\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=""center"">\n<a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-blue.svg"" alt=""License: MIT""></a>\n<a href=""https://discord.gg/DYn29wFk9z""><img src=""https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat"" alt=""Discord Follow""></a>\n<a href=""https://twitter.com/MetaGPT_""><img src=""https://img.shields.io/twitter/follow/MetaGPT?style=social"" alt=""Twitter Follow""></a>\n</p>\n\n<h4 align=""center"">\n\n</h4>\n\n## News\n\nüöÄ Mar. 10, 2025: üéâ [mgx.dev](https://mgx.dev/) is the #1 Product of the Week on @ProductHunt! üèÜ\n\nüöÄ Mar. &nbsp; 4, 2025: üéâ [mgx.dev](https://mgx.dev/) is the #1 Product of the Day on @ProductHunt! üèÜ\n\nüöÄ Feb. 19, 2025: Today we are officially launching our natural language programming product: [MGX (MetaGPT X)](https://mgx.dev/) - the world\'s first AI agent development team. More details on [Twitter](https://x.com/MetaGPT_/status/1892199535130329356).\n\nüöÄ Feb. 17, 2025: We introduced two papers: [SPO](https://arxiv.org/pdf/2502.06855) and [AOT](https://arxiv.org/pdf/2502.12018), check the [code](examples)!\n\nüöÄ Jan. 22, 2025: Our paper [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) accepted for **oral presentation (top 1.8%)** at ICLR 2025, **ranking #2** in the LLM-based Agent category.\n\nüëâüëâ [Earlier news](docs/NEWS.md)\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=""center"">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: `python --version`.\n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\n**Install [node](https://nodejs.org/en/download) and [pnpm](https://pnpm.io/installation#using-npm) before actual use.']","SPO is one of the two papers introduced by MetaGPT on Feb. 17, 2025. The details can be found in the paper available at [https://arxiv.org/pdf/2502.06855](https://arxiv.org/pdf/2502.06855).",single_hop_specifc_query_synthesizer
"What is the role of a Data Interpreter in MetaGPT, and how can it be used to perform tasks such as data analysis?","['**\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: ""openai""  # or azure / ollama / groq etc. Check LLMType for more options\n  model: ""gpt-4-turbo""  # or gpt-3.5-turbo\n  base_url: ""https://api.openai.com/v1""  # or forward url / other llm url\n  api_key: ""YOUR_API_KEY""\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt ""Create a 2048 game""  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo\nfrom metagpt.utils.project_repo import ProjectRepo\n\nrepo: ProjectRepo = generate_repo(""Create a 2048 game"")  # or ProjectRepo(""<path>"")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(""Run data analysis on sklearn Iris dataset, include a plot"")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```']","A Data Interpreter in MetaGPT is used to write code and perform tasks such as data analysis. For example, you can use the Data Interpreter to run data analysis on the sklearn Iris dataset and include a plot by writing code in Python. The process involves importing the DataInterpreter class from metagpt.roles.di.data_interpreter, creating an instance of DataInterpreter, and then calling the run method with the task description. The code can be executed asynchronously using asyncio.run or await in a Jupyter notebook setting.",single_hop_specifc_query_synthesizer
"How does AutoGen Studio facilitate the creation of multi-agent workflows without requiring coding, and how does it compare to CrewAI in terms of workflow management?","['<1-hop>\n\n# NOTE: you can skip input by pressing Enter.\n    user_proxy = UserProxyAgent(""user_proxy"")\n    # The termination condition is set to end the conversation when the user types \'exit\'.\n    termination = TextMentionTermination(""exit"", sources=[""user_proxy""])\n    # Web surfer and user proxy take turns in a round-robin fashion.\n    team = RoundRobinGroupChat([web_surfer, user_proxy], termination_condition=termination)\n    try:\n        # Start the team and wait for it to terminate.\n        await Console(team.run_stream(task=""Find information about AutoGen and write a short summary.""))\n    finally:\n        await web_surfer.close()\n        await model_client.close()\n\nasyncio.run(main())\n```\n\n### AutoGen Studio\n\nUse AutoGen Studio to prototype and run multi-agent workflows without writing code.\n\n```bash\n# Run AutoGen Studio on http://localhost:8080\nautogenstudio ui --port 8080 --appdir ./my-app\n```\n\n## Why Use AutoGen?\n\n<div align=""center"">\n  <img src=""autogen-landing.jpg"" alt=""AutoGen Landing"" width=""500"">\n</div>\n\nThe AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.\n\nThe _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.\n\n- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.\n- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated\xa0API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.\n- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.\n\nThe ecosystem also supports two essential _developer tools_:\n\n<div align=""center"">\n  <img src=""https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png"" alt=""AutoGen Studio Screenshot"" width=""500"">\n</div>\n\n- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.\n- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.\n\nYou can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.\n\nWith AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and updates.\n\n## Where to go next?\n\n<div align=""center"">\n\n|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&logoColor=white)](./dotnet) | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&logoColor=white)](./python/packages/autogen-studio)                     |\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                            | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html) | [!', ""<2-hop>\n\nUse Flow decorators to manage the sequence of operations\n4. Implement conditional branching based on Crew results\n\n## Connecting Your Crew to a Model\n\nCrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.\n\nPlease refer to the [Connect CrewAI to LLMs](https://docs.crewai.com/how-to/LLM-Connections/) page for details on configuring your agents' connections to models.\n\n## How CrewAI Compares\n\n**CrewAI's Advantage**: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.\n\n- **LangGraph**: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework's tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.\n\n*P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example ([see comparison](https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent)) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example ([detailed analysis](https://github.com/crewAIInc/crewAI-examples/blob/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb)).*\n\n- **Autogen**: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.\n\n- **ChatDev**: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.\n\n## Contribution\n\nCrewAI is open-source and we welcome contributions. If you're looking to contribute, please:\n\n- Fork the repository.\n- Create a new branch for your feature.\n- Add your feature or improvement.\n- Send a pull request.\n- We appreciate your input!\n\n### Installing Dependencies\n\n```bash\nuv lock\nuv sync\n```\n\n### Virtual Env\n\n```bash\nuv venv\n```\n\n### Pre-commit hooks\n\n```bash\npre-commit install\n```\n\n### Running Tests\n\n```bash\nuv run pytest .\n```\n\n### Running static type checks\n\n```bash\nuvx mypy src\n```\n\n### Packaging\n\n```bash\nuv build\n```\n\n### Installing Locally\n\n```bash\npip install dist/*.tar.gz\n```\n\n## Telemetry\n\nCrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.\n\nIt's pivotal to understand that **NO data is collected** concerning prompts, task descriptions, agents' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the `share_crew` feature is enabled, detailed data including task descriptions, agents' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.\n\nData collected includes:\n\n- Version of CrewAI\n  - So we can understand how many users are using the latest version\n- Version of Python\n  - So we can decide on what versions to better support\n- General OS (e.g.""]","AutoGen Studio allows users to prototype and run multi-agent workflows without writing code by providing a no-code GUI interface. It is part of the AutoGen ecosystem, which includes a framework with layered and extensible design, developer tools, and applications. AutoGen excels in creating conversational agents but lacks an inherent concept of process, requiring additional programming for orchestrating agent interactions. In contrast, CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture, offering both high-level orchestration and low-level customization. CrewAI demonstrates significant performance advantages and is more suited for complex, production-grade systems with granular control compared to AutoGen.",multi_hop_specific_query_synthesizer
What are the key features of CrewAI Enterprise Suite and how does it differ from the standard CrewAI framework?,"[""<1-hop>\n\nUsers can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.\n\nData collected includes:\n\n- Version of CrewAI\n  - So we can understand how many users are using the latest version\n- Version of Python\n  - So we can decide on what versions to better support\n- General OS (e.g. number of CPUs, macOS/Windows/Linux)\n  - So we know what OS we should focus on and if we could build specific OS related features\n- Number of agents and tasks in a crew\n  - So we make sure we are testing internally with similar use cases and educate people on the best practices\n- Crew Process being used\n  - Understand where we should focus our efforts\n- If Agents are using memory or allowing delegation\n  - Understand if we improved the features or maybe even drop them\n- If Tasks are being executed in parallel or sequentially\n  - Understand if we should focus more on parallel execution\n- Language model being used\n  - Improved support on most used languages\n- Roles of agents in a crew\n  - Understand high level use cases so we can build better tools, integrations and examples about it\n- Tools names available\n  - Understand out of the publicly available tools, which ones are being used the most so we can improve them\n\nUsers can opt-in to Further Telemetry, sharing the complete telemetry data by setting the `share_crew` attribute to `True` on their Crews. Enabling `share_crew` results in the collection of detailed crew and task execution data, including `goal`, `backstory`, `context`, and `output` of tasks. This enables a deeper insight into usage patterns while respecting the user's choice to share.\n\n## License\n\nCrewAI is released under the [MIT License](https://github.com/crewAIInc/crewAI/blob/main/LICENSE).\n\n\n## Frequently Asked Questions (FAQ)\n\n### General\n- [What exactly is CrewAI?](#q-what-exactly-is-crewai)\n- [How do I install CrewAI?](#q-how-do-i-install-crewai)\n- [Does CrewAI depend on LangChain?](#q-does-crewai-depend-on-langchain)\n- [Is CrewAI open-source?](#q-is-crewai-open-source)\n- [Does CrewAI collect data from users?](#q-does-crewai-collect-data-from-users)\n\n### Features and Capabilities\n- [Can CrewAI handle complex use cases?](#q-can-crewai-handle-complex-use-cases)\n- [Can I use CrewAI with local AI models?](#q-can-i-use-crewai-with-local-ai-models)\n- [What makes Crews different from Flows?](#q-what-makes-crews-different-from-flows)\n- [How is CrewAI better than LangChain?](#q-how-is-crewai-better-than-langchain)\n- [Does CrewAI support fine-tuning or training custom models?](#q-does-crewai-support-fine-tuning-or-training-custom-models)\n\n### Resources and Community\n- [Where can I find real-world CrewAI examples?](#q-where-can-i-find-real-world-crewai-examples)\n- [How can I contribute to CrewAI?](#q-how-can-i-contribute-to-crewai)\n\n### Enterprise Features\n- [What additional features does CrewAI Enterprise offer?](#q-what-additional-features-does-crewai-enterprise-offer)\n- [Is CrewAI Enterprise available for cloud and on-premise deployments?](#q-is-crewai-enterprise-available-for-cloud-and-on-premise-deployments)\n- [Can I try CrewAI Enterprise for free?](#q-can-i-try-crewai-enterprise-for-free)\n\n\n\n### Q: What exactly is CrewAI?\nA: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.\n\n### Q: How do I install CrewAI?\nA: Install CrewAI using pip:\n```shell\npip install crewai\n```\nFor additional tools, use:\n```shell\npip install 'crewai[tools]'\n```\n### Q: Does CrewAI depend on LangChain?\nA: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.\n\n### Q: Can CrewAI handle complex use cases?\nA: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration."", '<2-hop>\n\n<div align=""center"">\n\n![Logo of CrewAI](./docs/crewai_logo.png)\n\n\n</div>\n\n### Fast and Flexible Multi-Agent Automation Framework\n\nCrewAI is a lean, lightning-fast Python framework built entirely from\nscratch‚Äîcompletely **independent of LangChain or other agent frameworks**.\nIt empowers developers with both high-level simplicity and precise low-level\ncontrol, ideal for creating autonomous AI agents tailored to any scenario.\n\n- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.\n- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively\n\nWith over 100,000 developers certified through our community courses at\n[learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the\nstandard for enterprise-ready AI automation.\n\n# CrewAI Enterprise Suite\n\nCrewAI Enterprise Suite is a comprehensive bundle tailored for organizations\nthat require secure, scalable, and easy-to-manage agent-driven automation.\n\nYou can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)\n\n## Crew Control Plane Key Features:\n- **Tracing & Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.\n- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.\n- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.\n- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.\n- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.\n- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.\n- **On-premise and Cloud Deployment Options**: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.\n\nCrewAI Enterprise is designed for enterprises seeking a powerful,\nreliable solution to transform complex business processes into efficient,\nintelligent automations.\n\n<h3>\n\n[Homepage](https://www.crewai.com/) | [Documentation](https://docs.crewai.com/) | [Chat with Docs](https://chatg.pt/DWjSBZn) | [Discourse](https://community.crewai.com)\n\n</h3>\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/joaomdmoura/crewAI)](https://github.com/crewAIInc/crewAI)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\n</div>\n\n## Table of contents\n\n- [Why CrewAI?](#why-crewai)\n- [Getting Started](#getting-started)\n- [Key Features](#key-features)\n- [Understanding Flows and Crews](#understanding-flows-and-crews)\n- [CrewAI vs LangGraph](#how-crewai-compares)\n- [Examples](#examples)\n  - [Quick Tutorial](#quick-tutorial)\n  - [Write Job Descriptions](#write-job-descriptions)\n  - [Trip Planner](#trip-planner)\n  - [Stock Analysis](#stock-analysis)\n  - [Using Crews and Flows Together](#using-crews-and-flows-together)\n- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)\n- [How CrewAI Compares](#how-crewai-compares)\n- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)\n- [Contribution](#contribution)\n- [Telemetry](#telemetry)\n- [License](#license)\n\n## Why CrewAI?\n\n<div align=""center"" style=""margin-bottom: 30px;"">\n  <img src=""docs/asset.png"" alt=""CrewAI Logo"" width=""100%"">\n</div>\n\nCrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:\n\n- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.\n- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.\n- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.\n- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.']","The CrewAI Enterprise Suite offers features like Tracing & Observability, Unified Control Plane, Seamless Integrations, Advanced Security, Actionable Insights, 24/7 Support, and On-premise and Cloud Deployment Options. It is tailored for organizations needing secure, scalable, and easy-to-manage agent-driven automation. In contrast, the standard CrewAI framework is a lean, fast Python framework built from scratch, independent of LangChain, focusing on high performance and flexible low-level customization for both simple and complex use cases.",multi_hop_specific_query_synthesizer
